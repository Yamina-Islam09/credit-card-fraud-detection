# -*- coding: utf-8 -*-
"""Copy of credit card fraud(full dataset).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CgOKTR_ZiIMEAzvjfLHrzQtjs5cQsY6z
"""

import tensorflow as tf
tf.test.gpu_device_name()

# Import models and utility functions
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.metrics import mean_squared_error as MSE
from sklearn.metrics import roc_auc_score, accuracy_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.lines as mlines
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

df = pd.read_csv('PS_20174392719_1491204439457_log.csv',header=0)
df.head(n=10)
df

null_values = df.isnull().sum()

# Display the result
print(null_values)

df.describe()

# Remove 'nameOrig' and 'nameDest' columns from the dataset
df = df.drop(columns=['nameOrig', 'nameDest'])

# Check the new dataset after removing the columns
print(df.head())
df

# Create a boxplot comparing 'step' by 'isFraud'
plt.figure(figsize=(10,6))  # Set figure size
sns.boxplot(x='isFraud', y='step', data=df, palette='Set2')

# Add titles and labels
plt.title('Step vs. Fraud Classification')
plt.xlabel('Fraud Classification (0 = Non-Fraud, 1 = Fraud)')
plt.ylabel('Step')
plt.savefig('step_vs_fraud_boxplot.png', dpi=300, bbox_inches='tight')
# Show the plot
plt.show()

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

# Apply Label Encoding to 'type' columns
df['type'] = label_encoder.fit_transform(df['type'])

df

from sklearn.preprocessing import MinMaxScaler

# Assuming your dataset is loaded as 'df'

# Create an instance of MinMaxScaler
scaler = MinMaxScaler()

# List of numerical columns to scale
numerical_columns = ['step','amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']

# Apply Min-Max Scaling to the numerical columns
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])
df

# Print counts with labels
counts = df['isFraud'].value_counts()
print(f"Non-Fraudulent Transactions (0): {counts.get(0, 0)}")
print(f"Fraudulent Transactions (1): {counts.get(1, 0)}")

import matplotlib.pyplot as plt
import seaborn as sns

# Plot the distribution of the target variable 'isFraud'
plt.figure(figsize=(8, 6))
sns.countplot(x='isFraud', data=df, palette='Set2')

# Add titles and labels
plt.title('Distribution of Fraudulent vs Non-Fraudulent Transactions')
plt.xlabel('Is Fraud')
plt.ylabel('Count')
plt.savefig('Distribution of Fraudulent vs Non-Fraudulent Transactions.png', dpi=300, bbox_inches='tight')
# Show the plot
plt.show()

from imblearn.combine import SMOTETomek
from sklearn.model_selection import train_test_split

# Assuming 'df' is your DataFrame and 'isFraud' is the target column
# data.drop(columns=['isFraud', 'isFlaggedFraud'])
X = df.drop(columns=['isFraud', 'isFlaggedFraud'])  # Features
y = df['isFraud']  # Target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE-Tomek
smote_tomek = SMOTETomek(random_state=42)
X_res, y_res = smote_tomek.fit_resample(X_train, y_train)

# Check the class distribution after applying SMOTE-Tomek
print("Class distribution after SMOTE-Tomek:")
print(pd.Series(y_res).value_counts())
print("Class distribution after SMOTE-Tomek:")
print(pd.Series(y_test).value_counts())

# Plot before resampling
plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
y_train.value_counts().plot(kind='bar', title='Class Distribution before Resampling')
plt.xlabel('Class')
plt.ylabel('Count')
plt.savefig('Class Distribution before SMOTE-Tomek.png', dpi=300, bbox_inches='tight')
# Plot after SMOTE-Tomek
plt.subplot(1, 2, 2)
pd.Series(y_res).value_counts().plot(kind='bar', title='Class Distribution after SMOTE-Tomek')
plt.xlabel('Class')
plt.ylabel('Count')
plt.savefig('Class Distribution after SMOTE-Tomek.png', dpi=300, bbox_inches='tight')
plt.tight_layout()
plt.show()

print(f"Number of samples in the resampled training set (after SMOTE-Tomek): {X_res.shape[0]}")
print(f"Number of samples in the resampled training set (after SMOTE-Tomek): {X_test.shape[0]}")

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score

# KNN Model
knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors as needed
knn.fit(X_res, y_res)  # Train the KNN model on the resampled data
knn_train_accuracy = knn.score(X_res, y_res)  # Training accuracy
knn_test_accuracy = knn.score(X_test, y_test)  # Test accuracy
print("KNN Training Accuracy:", knn_train_accuracy)
print("KNN Testing Accuracy:", knn_test_accuracy)

# Decision Tree Model
dt = DecisionTreeClassifier(random_state=42,max_depth=50,min_samples_split=8)  # You can tune parameters like max_depth, min_samples_split
dt.fit(X_res, y_res)  # Train the Decision Tree model on the resampled data
dt_train_accuracy = dt.score(X_res, y_res)  # Training accuracy
dt_test_accuracy = dt.score(X_test, y_test)  # Test accuracy
print("Decision Tree Training Accuracy:", dt_train_accuracy)
print("Decision Tree Testing Accuracy:", dt_test_accuracy)

# Random Forest Model
rf = RandomForestClassifier(random_state=42,max_depth=50,min_samples_split=8)  # You can tune parameters like n_estimators, max_depth
rf.fit(X_res, y_res)  # Train the Random Forest model on the resampled data
rf_train_accuracy = rf.score(X_res, y_res)  # Training accuracy
rf_test_accuracy = rf.score(X_test, y_test)  # Test accuracy
print("Random Forest Training Accuracy:", rf_train_accuracy)
print("Random Forest Testing Accuracy:", rf_test_accuracy)

ensemble_model = VotingClassifier(estimators=[
    ('knn', knn),
    ('dt', dt),
    ('rf', rf)
], voting='soft')

# Train the ensemble model on the resampled data
ensemble_model.fit(X_res, y_res)

# Training Accuracy
ensemble_train_accuracy = ensemble_model.score(X_res, y_res)

# Test Accuracy
ensemble_test_accuracy = ensemble_model.score(X_test, y_test)

# Print the training and test accuracy for the ensemble model
print("Ensemble Model Training Accuracy:", ensemble_train_accuracy)
print("Ensemble Model Testing Accuracy:", ensemble_test_accuracy)

# 1. Evaluate individual models (RF, DT, KNN) with the best parameters found by GridSearchCV
# Evaluate Random Forest
from sklearn.metrics import precision_score, recall_score, f1_score
rf_pred = rf.predict(X_test)
rf_precision = precision_score(y_test, rf_pred, average='weighted')
rf_recall = recall_score(y_test, rf_pred, average='weighted')
rf_f1 = f1_score(y_test, rf_pred, average='weighted')

# Evaluate Decision Tree
dt_pred = dt.predict(X_test)
dt_precision = precision_score(y_test, dt_pred, average='weighted')
dt_recall = recall_score(y_test, dt_pred, average='weighted')
dt_f1 = f1_score(y_test, dt_pred, average='weighted')

# Evaluate K-Nearest Neighbors
knn_pred = knn.predict(X_test)
knn_precision = precision_score(y_test, knn_pred, average='weighted')
knn_recall = recall_score(y_test, knn_pred, average='weighted')
knn_f1 = f1_score(y_test, knn_pred, average='weighted')

# 2. Print the evaluation metrics for individual models
print("Random Forest Evaluation:")
print(f"Precision: {rf_precision:.4f}")
print(f"Recall: {rf_recall:.4f}")
print(f"F1-Score: {rf_f1:.4f}")

print("\nDecision Tree Evaluation:")
print(f"Precision: {dt_precision:.4f}")
print(f"Recall: {dt_recall:.4f}")
print(f"F1-Score: {dt_f1:.4f}")

print("\nK-Nearest Neighbors Evaluation:")
print(f"Precision: {knn_precision:.4f}")
print(f"Recall: {knn_recall:.4f}")
print(f"F1-Score: {knn_f1:.4f}")

y_pred_test = ensemble_model.predict(X_test)
ensemble_test_precision = precision_score(y_test, y_pred_test, average='weighted')
ensemble_test_recall = recall_score(y_test, y_pred_test, average='weighted')
ensemble_test_f1 = f1_score(y_test, y_pred_test, average='weighted')

print("\nEnsemble Model Testing Precision:", ensemble_test_precision)
print("Ensemble Model Testing Recall:", ensemble_test_recall)
print("Ensemble Model Testing F1-Score:", ensemble_test_f1)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have already trained your ensemble model
# For example, ensemble_model is the trained VotingClassifier

# Make predictions on the test data
y_pred = ensemble_model.predict(X_test)  # X_test is your test set

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)  # y_test is the true labels for the test set

# Visualize the confusion matrix using Seaborn heatmap
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

from sklearn.model_selection import KFold
from sklearn.metrics import make_scorer
from sklearn.model_selection import cross_validate,cross_val_predict
kf = KFold(n_splits=5, shuffle=True, random_state=42)

scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score,average='weighted'),
    'recall': make_scorer(recall_score,average='weighted'),
    'f1': make_scorer(f1_score,average='weighted')
}

scores = cross_validate(ensemble_model, X, y, cv=kf, scoring=scoring,return_train_score=True)
y_pred = cross_val_predict(ensemble_model, X, y, cv=kf)

# Compute confusion matrix
cm = confusion_matrix(y, y_pred)

# Plot confusion matrix
plt.figure(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()
print("Accuracies for each fold:", scores['test_accuracy'])
print("Precisions for each fold:", scores['test_precision'])
print("Recalls for each fold:", scores['test_recall'])
print("F1 scores for each fold:", scores['test_f1'])

print("Accuracies for each fold:", scores['train_accuracy'])
print("Precisions for each fold:", scores['train_precision'])
print("Recalls for each fold:", scores['train_recall'])
print("F1 scores for each fold:", scores['train_f1'])

# Split the dataset into training and test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. Hyperparameter tuning using GridSearchCV
# param_grid_rf = {'n_estimators': [50, 100, 200],
#     'max_depth': [5, 10, 15, None],
#     'min_samples_split': [2, 5, 10],
#     'min_samples_leaf': [1, 2, 4]}

# param_grid_dt = {'max_depth': [5, 10, 15, None],
#     'min_samples_split': [2, 5, 10],
#     'min_samples_leaf': [1, 2, 4],
#     'criterion': ['gini', 'entropy']}
# param_grid_knn = {'n_neighbors': [3, 5, 7, 9],
#     'weights': ['uniform', 'distance'],
#     'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
#     'leaf_size': [20, 30, 40],
#     'p': [1, 2]}

# # Create the classifiers
# rf1 = RandomForestClassifier()
# dt1 = DecisionTreeClassifier()
# knn1 = KNeighborsClassifier()

# Apply GridSearchCV
# grid_search_rf = GridSearchCV(estimator=rf1, param_grid=param_grid_rf, cv=5)
# grid_search_dt = GridSearchCV(estimator=dt1, param_grid=param_grid_dt, cv=5)
# grid_search_knn = GridSearchCV(estimator=knn1, param_grid=param_grid_knn, cv=5)

# # Fit GridSearchCV to find best hyperparameters
# grid_search_rf.fit(X_res, y_res)
# grid_search_dt.fit(X_res, y_res)
# grid_search_knn.fit(X_res, y_res)

# # Best parameters
# print("Best parameters for RF:", grid_search_rf.best_params_)
# print("Best parameters for DT:", grid_search_dt.best_params_)
# print("Best parameters for KNN:", grid_search_knn.best_params_)

# print(grid_search_rf.score(X_res, y_res),
# grid_search_dt.score(X_res, y_res),
# grid_search_knn.score(X_res, y_res))

# print(grid_search_rf.score(X_test, y_test),
# grid_search_dt.score(X_test, y_test),
# grid_search_knn.score(X_test, y_test))

# 2. Hyperparameter tuning using PSO (for RandomForest only as an example)
# def objective_function(params):
#     n_estimators, max_depth = params
#     rf = RandomForestClassifier(n_estimators=int(n_estimators), max_depth=int(max_depth))
#     rf.fit(X_train, y_train)
#     return -rf.score(X_test, y_test)  # Negative because PSO minimizes the function

# # PSO optimization for Random Forest
# lb = [50, 5]  # Lower bounds for n_estimators, max_depth
# ub = [500, 50]  # Upper bounds for n_estimators, max_depth
# best_params_rf, best_score = pso(objective_function, lb, ub, swarmsize=10, maxiter=5)
# print(f"Best RF parameters found by PSO: n_estimators={int(best_params_rf[0])}, max_depth={int(best_params_rf[1])}")

# 3. Train the models with the best parameters found by GridSearchCV and PSO
# rf_best = RandomForestClassifier(n_estimators=grid_search_rf.best_params_['n_estimators'],
#                                  max_depth=grid_search_rf.best_params_['max_depth'])
# rf_best.fit(X_res, y_res)

# dt_best = DecisionTreeClassifier(max_depth=grid_search_dt.best_params_['max_depth'],
#                                  min_samples_split=grid_search_dt.best_params_['min_samples_split'])
# dt_best.fit(X_res, y_res)

# knn_best = KNeighborsClassifier(n_neighbors=grid_search_knn.best_params_['n_neighbors'],
#                                 weights=grid_search_knn.best_params_['weights'])
# knn_best.fit(X_res, y_res)

# rf_best = RandomForestClassifier(n_estimators=grid_search_rf.best_params_,
#                                  max_depth=grid_search_rf.best_params_)
# rf_best.fit(X_res, y_res)
# rf_train_accuracy1 = rf_best.score(X_res, y_res)  # Training accuracy
# rf_test_accuracy1 = rf_best.score(X_test, y_test)  # Test accuracy
# print("RF Training Accuracy:", rf_train_accuracy1)
# print("RF Testing Accuracy:", rf_test_accuracy1)


# dt_best = DecisionTreeClassifier(max_depth=grid_search_dt.best_params_,
#                                  min_samples_split=grid_search_dt.best_params_)
# dt_best.fit(X_res, y_res)
# dt_train_accuracy1 = dt_best.score(X_res, y_res)  # Training accuracy
# dt_test_accuracy1 = dt_best.score(X_test, y_test)  # Test accuracy
# print("DT Training Accuracy:", dt_train_accuracy1)
# print("DT Testing Accuracy:", dt_test_accuracy1)


# knn_best = KNeighborsClassifier(n_neighbors=grid_search_knn.best_params_,
#                                 weights=grid_search_knn.best_params_)
# knn_best.fit(X_res, y_res)
# knn_train_accuracy1 = knn_best.score(X_res, y_res)  # Training accuracy
# knn_test_accuracy1 = knn_best.score(X_test, y_test)  # Test accuracy
# print("KNN Training Accuracy:", knn_train_accuracy1)
# print("KNN Testing Accuracy:", knn_test_accuracy1)

# # 4. Ensemble the models using Voting Classifier
# voting_clf = VotingClassifier(estimators=[
#     ('rf', rf_best),
#     ('dt', dt_best),
#     ('knn', knn_best)
# ], voting='soft')  # 'hard' voting means majority voting (for classification)

# # Train the Voting Classifier
# voting_clf.fit(X_train, y_train)

# # 5. Evaluate the ensemble model
# y_pred = voting_clf.predict(X_test)
# print("Ensemble model accuracy:", accuracy_score(y_test, y_pred))

pip install lime shap

import shap


# Reduce the background data size to a smaller number of representative samples
# background_data = shap.sample(X_res,10)  # Use 1000 random samples from the training data
x_test_subset = X_test.sample(n=100, random_state=42)
# Use the smaller background data for SHAP explanation
# explainer_shap = shap.KernelExplainer(ensemble_model.predict_proba, background_data)

ens_explainer = shap.KernelExplainer(ensemble_model.predict_proba, x_test_subset)
shap_values_ens = ens_explainer.shap_values(x_test_subset)

shap_values_ens

# Assuming shap_values_ens holds the SHAP values for the ensemble model
# Extract the SHAP values for the specific instance (e.g., sample index 42)
sample_index = 42  # Choose the sample index you want to explain

# SHAP values for class 1 (fraudulent class)
shap_values_single = shap_values_ens[sample_index]  # Adjust if the data structure is different

shap.summary_plot(shap_values_ens[:,:,0], x_test_subset)
plt.savefig('shap_summary_plot.jpg', dpi=300, bbox_inches='tight')

print(f"Shape of SHAP values: {shap_values_ens.shape}")
print(f"Shape of X_test: {x_test_subset.shape}")

shap.summary_plot(shap_values_ens[:,:,0], x_test_subset, plot_type="violin")
plt.savefig('shap_summary_plot1.jpg', dpi=300, bbox_inches='tight')

x_test_subset.columns

# Plot the summary plot for Class 0 (non-fraud)
shap.summary_plot(shap_values_ens[:,:,0],x_test_subset, plot_type="bar")
plt.savefig('shap_mean_summary_plot.jpg', dpi=300, bbox_inches='tight')

shap.summary_plot(shap_values_ens[:,:,0], x_test_subset)
plt.savefig('shap_summary_plot3.jpg', dpi=300, bbox_inches='tight')

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Get predicted probabilities for the positive class (class 1)
knn_probs = knn.predict_proba(X_test)[:, 1]
dt_probs = dt.predict_proba(X_test)[:, 1]
rf_probs = rf.predict_proba(X_test)[:, 1]
ensemble_probs = ensemble_model.predict_proba(X_test)[:, 1]

# Compute ROC curve and AUC for each model
fpr_knn, tpr_knn, _ = roc_curve(y_test, knn_probs)
auc_knn = auc(fpr_knn, tpr_knn)

fpr_dt, tpr_dt, _ = roc_curve(y_test, dt_probs)
auc_dt = auc(fpr_dt, tpr_dt)

fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)
auc_rf = auc(fpr_rf, tpr_rf)

fpr_ens, tpr_ens, _ = roc_curve(y_test, ensemble_probs)
auc_ens = auc(fpr_ens, tpr_ens)

# Plot all ROC curves
plt.figure(figsize=(10, 8))
plt.plot(fpr_knn, tpr_knn, label=f'KNN (AUC = {auc_knn:.2f})')
plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {auc_dt:.2f})')
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.2f})')
plt.plot(fpr_ens, tpr_ens, label=f'Ensemble (AUC = {auc_ens:.2f})', linewidth=2, color='black')

# Plot the random classifier line
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')

# Plot settings
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend(loc='lower right')

# Save the plot
plt.savefig('roc_curve_all_models.png', dpi=300, bbox_inches='tight')
plt.show()

X_test, y_test = smote_tomek.fit_resample(X_test, y_test) #after applying smote-tomek test values
X_test,y_test

print(pd.Series(y_test).value_counts())